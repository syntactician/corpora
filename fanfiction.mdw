% TODO:
% * finish ff_spider

\documentclass[man,12pt]{apa6}
\usepackage[colorlinks=true]{hyperref}
\usepackage{amssymb,amsmath,times,minted}
\linespread{1.5}

\begin{document}

\title{Homework 3}
\shorttitle{Homework 3}
\author{Edward Hern\'{a}ndez}
\date{23 March 2016}
\affiliation{College of William \& Mary}
\maketitle

\section{The Program}

This document is a
\href{http://www.literateprogramming.com/knuthweb.pdf}{literate} Python program
inset within a \href{https://www.latex-project.org}{\LaTeX} document, woven and
executed using \href{http://mpastell.com/pweave/}{Pweave} and typeset using
\href{https://www.tug.org/applications/pdftex}{pdf{\TeX}} This code is written
in/for Python 3, and is intended for and tested under only
\href{https://www.python.org/downloads/releases/351}{version 3.5.1}.  It uses
\href{http://www.nltk.org}{nltk} to analyze corpora built using
\href{http://scrapy.org}{Scrapy}
(\href{https://blog.scrapinghub.com/2016/02/04/python-3-support-with-scrapy-1-1rc1/}{$\geq$1.1.0rc1}).\footnote{
	Scrapy's stable release
	(\href{https://pypi.python.org/pypi/Scrapy/1.1.0rc3}{1.0.5}) does not
	support Python 3, so this program currently runs against only development
	versions after 1.1.0rc1. All code is tested to run against 1.1.0rc1 but is
	primarily run against 1.2.0dev2, the current development branch on github.
}

Resolving the dependencies for this code is non-trivial.
Ideally, \href{pip} should be able to handle the dependencies:

\inputminted{bash}{pip.sh}

\noindent

There are some issues with using pip, however. If you are using an operating
system with a package manager, you \href{}{may} wish to allow that package
manager to handle the packages which constitute your Python environment.
%
If this is your preference, this code presents an unusual complication. It
relies on both bleeding-edge versions of several Python packages (likely to be
ahead of the stable versions packaged for your OS) and on some Python modules
that are not packaged for most\footnote{
	Arch Linux is likely the only exception, as I have personally published
	\href{}{packages} to satisfy all dependencies the distribution lacked.
} operating systems.
%
As a result, you may not want to attempt to alter your Python installation to
accomodate this program.
%
Additionally, if you are running OS X, pip will likely fail building
\href{}{lxml} (a dependency of Scrapy), since OS X does not ship libxml by
default. This can usually be solved by (re)installing the Xcode command line
tools: \mintinline{bash}{xcode-select install}. However, this is not always
successful.  

In either case, the best solution is probably virtualization. The following
Dockerfiles are minimally sufficient to create suitable environments in Ubuntu
16.04 Xerial Xerus (which must still rely on pip) and Arch Linux (which is
altogether more intensive to set up), respectively.\footnote{
	Virtualenv is another good solution to avoid altering your Python
	installation, but cannot be used to satisfy the dependence on libxml, since
	it manages only Python-internal dependencies.
}

Ubuntu:

\inputminted{docker}{ubuntu.docker}

Arch Linux:

\inputminted{docker}{arch.docker}

% This Dockerfile installs the following dependencies, against which the code has
% been tested to run on Ubuntu 16.04:

% \inputminted{python}{requirements.txt}



\section{Building Corpora with \href{http://scrapy.org}{Scrapy}}

Import stuff:

% import {{{
<<import>>=
from chatterbot import ChatBot
from datetime import datetime
from glob import glob
from json import loads
from nltk import ConditionalFreqDist
from nltk import FreqDist
from nltk.corpus import stopwords
from nltk.data import load
from nltk.tokenize import word_tokenize
from scrapy import Field
from scrapy import Item
from scrapy import Request
from scrapy import Spider
from scrapy import signals
from scrapy.crawler import Crawler
from scrapy.exporters import JsonLinesItemExporter
from scrapy.loader import ItemLoader
from scrapy.loader.processors import Join
from scrapy.loader.processors import MapCompose
from scrapy.loader.processors import TakeFirst
from scrapy.settings import Settings
from scrapy.utils import log
from twisted.internet import reactor
from w3lib.html import remove_tags
import json
import re
@
% }}}

Scrapy provides an
\texttt{\href{http://doc.scrapy.org/en/latest/topics/items.html}{Item}} class
which is used to collect scraped data. The following code defines an
\texttt{StoryItem} class to hold information about stories. Each chapter of
each story in the corpus will be stored as a separate text (\texttt{body}) with
a \texttt{title}, an \texttt{author}, a description or summary (\texttt{desc}),
the topic or \texttt{theme} of the story, its \texttt{page} or chapter number
within a larger work, the \texttt{site} it was scraped from, and the
\texttt{url} from which it was scraped.

% I may further define items for each site (inheriting the StoryItem class) for
% extra data

% story_item {{{
<<story_item>>=
class StoryItem(Item):
    title    = Field()
    author   = Field()
    desc     = Field()
    body     = Field()
    url      = Field()
    site     = Field()
    chapter  = Field()
    category = Field()
@
% }}}

% le_item {{{
<<le_item>>=
@
% }}}

% ff_item {{{
<<ff_item>>=
class FFItem(StoryItem):
    id        = Field()
    rating    = Field()
    language  = Field()
    words     = Field()
    chapters  = Field()
    complete  = Field()
    comments  = Field()
    likes     = Field()
    marks     = Field()
    published = Field()
    updated   = Field()
@
% }}}

% ao_item {{{
<<ao_item>>=
class AOItem(FFItem):
    fandom     = Field()
    characters = Field()
    ships      = Field()
    tags       = Field()
    warnings   = Field()
    hits       = Field()
@
% }}}

% consider putting below spiders
Scrapy provides an API by which data can be loaded into an \texttt{Item} via an
\texttt{\href{http://doc.scrapy.org/en/latest/topics/loaders.html}{ItemLoader}}.
The code below specifies a \texttt{StoryItemLoader} (which inherits the
\texttt{ItemLoader} class defined by Scrapy) to load story data generally  and
two classes which inherit it, to load particular data from specific websites.

In cases where both the \texttt{StoryItemLoader} and one of the child classes
specify a field (e.g. \texttt{body\_out}), the child class supersedes the parent.

These loaders cannot be written before the \texttt{Spider}s defined below, as
they operate on the output of those Spiders (and thus depend necessarily for
their design on the design of those Spiders). They are defined here only
because they must exist to be called by the Spiders.

% item loaders {{{
<<item_loader>>=
class StoryItemLoader(ItemLoader):
    default_input_processor  = MapCompose(str.strip)
    default_output_processor = TakeFirst()

    body_in  = MapCompose(remove_tags)
    body_out = Join()

class FFItemLoader(StoryItemLoader):
    # desc_out = Join()
    desc_in  = MapCompose(remove_tags)
    desc_out = Join()

class AOItemLoader(StoryItemLoader):
    body_out       = Join()
    category_out   = Join(', ')
    warnings_out   = Join(', ')
    fandom_out     = Join(', ')
    characters_out = Join(', ')
    ships_out      = Join(', ')
    tags_out       = Join(', ')
@
% }}}

Pipe the scraped text into Json items on the lines of a file (one file per
spider). 
% consider putting below spiders

% pipeline {{{
<<pipeline>>=
class JsonLinesExportPipeline(object):
    def __init__(self):
        self.files = {}

    @classmethod
    def from_crawler(cls, crawler):
        pipeline = cls()
        crawler.signals.connect(
            pipeline.spider_opened,
            signals.spider_opened
        )
        crawler.signals.connect(
            pipeline.spider_closed,
            signals.spider_closed
        )
        return pipeline

    def spider_opened(self, spider):
        file = open('%s_stories.jl' % spider.name, 'w+b')
        self.files[spider] = file
        self.exporter = JsonLinesItemExporter(file)
        self.exporter.start_exporting()

    def spider_closed(self, spider):
        self.exporter.finish_exporting()
        file = self.files.pop(spider)
        file.close()

    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item
@
% }}}

A spider to scrape \texttt{\href{https://fanfiction.net}{fanfiction.net}}:
% fanfiction.net spider {{{
<<ffnet_spider>>=
class FFSpider(Spider):
    name = "ff"
    allowed_domains = ["fanfiction.net"]
    start_urls = [
        "https://www.fanfiction.net/%s/" % c for c in 
        (
            'anime',
            'book',
            'cartoon',
            'comic',
            'game',
            'misc',
            'movie',
            'play',
            'tv'
        )
    ]

    def parse(self, response):
        tags = response.xpath(
            '//td[@valign="TOP"]/div/a/@href'
        )
        for href in tags:
            url = response.urljoin(href.extract())
            yield Request(url, callback = self.parse_tag)

    def parse_tag(self, response):
        next = response.xpath(
            '//center[1]/a[contains(text(), "Next")]/@href'
        )
        for href in next:
            url = response.urljoin(href.extract())
            yield Request(url, callback = self.parse_tag)

        stories = response.xpath(
            '//div[contains(@class,"z-list")]/a[1]/@href'
        )
        for href in stories:
            long_url = response.urljoin(href.extract())
            url      = '/'.join(long_url.split('/')[0:-1])
            yield Request(url, callback = self.parse_story)

    def parse_story(self, response):
        header  = response.xpath(
            '//span[@class="xgray xcontrast_txt"]/text()'
        )
        head    = header.extract()
        chapter = int(response.url.split('/')[-1])
        more    = re.search('Chapters: [0-9]*', head[1])

        if more and chapter == 1:
            chapters = int(more.group(0).split()[1])
            base_url = '/'.join(response.url.split('/')[0:-1])
            urls = [
                base_url + '/' +  str(x) for x in range(2, chapters+1)
            ]
            for url in urls:
                yield Request(url, callback = self.parse_story)

        time_xpath  = response.xpath(
            '//span[@data-xutime]/@data-xutime'
        )
        times       = time_xpath.extract()
        u_published = float(times[0])
        d_published = datetime.fromtimestamp(u_published)
        published   = d_published.strftime('%Y-%m-%d')
        u_updated   = float(next(reversed(times)))
        d_updated   = datetime.fromtimestamp(u_updated)
        updated     = d_updated.strftime('%Y-%m-%d')

        # info    = head[1].split(' - ')
        # language = info[1]
        # category = info[2]
        # characters = info[3]
        # words = ''.join([s for s in info[5] if s.isdigit()])

        complete = str(bool('Complete' in ' '.join(head)))

        loader = FFItemLoader(FFItem(), response=response)
        loader.add_xpath(
            'title', '//*[@id="profile_top"]/b/text()'
        )
        loader.add_xpath(
            'author', '//*[@id="profile_top"]/a[1]/text()'
        )
        loader.add_xpath(
            'desc', '//*[@id="profile_top"]/div'
        )
        loader.add_xpath(
            'body', '//*[@id="storytext"]'
        )
        loader.add_value(
            'url', response.url
        )
        loader.add_value(
            'site', 'fanfiction.net'
        )
        loader.add_value(
            'chapter', str(chapter)
        )
        loader.add_xpath(
            'rating', '//span[@class="xgray xcontrast_txt"]/a/text()'
        )
        loader.add_value(
            'published', published
        )
        loader.add_value(
            'updated', updated
        )
        yield loader.load_item()
@
% }}}

Spider for \href{https://www.literotica.com/}{Literotica}:
% may produce roughly 602378 results... maybe more?

% literotica spider {{{
<<literotica_spider>>=
class LESpider(Spider):
    name = "le"
    allowed_domains = ["literotica.com"]
    start_urls = [
        "https://www.literotica.com/c/%s/1-page" % c for c in
        (
            'adult-how-to',
            'adult-humor',
            'adult-romance',
            'anal-sex-stories',
            'bdsm-stories',
            'bdsm-stories',
            'celebrity-stories',
            'chain-stories',
            'erotic-couplings',
            'erotic-horror',
            'erotic-letters',
            'erotic-novels',
            'erotic-poetry',
            'exhibitionist-voyeur',
            'fetish-stories',
            'first-time-sex-stories',
            'gay-sex-stories',
            'group-sex-stories',
            'illustrated-erotic-fiction',
            'interracial-erotic-fiction',
            'lesbian-sex-stories',
            'loving-wives',
            'masturbation-stories',
            'mature-sex',
            'mind-control',
            'non-consent-stories',
            'non-erotic-poetry',
            'non-erotic-stories',
            'non-human-stories',
            'reviews-and-essays',
            'science-fiction-fantasy',
            'taboo-sex-stories',
            'transsexuals-crossdressers'
        )
    ]

    def parse(self, response):
        next = response.xpath(
            '//*[@class="b-pager-next"]/@href'
        )
        for href in next:
            url = response.urljoin(href.extract())
            yield Request(url, callback = self.parse)

        stories = response.xpath(
            '//*[@id="content"]/div/div/h3/a/@href'
        )
        for href in stories:
            url = response.urljoin(href.extract())
            yield Request(url, callback = self.parse_story)

    def parse_story(self, response):
        next = response.xpath(
            '//*[@class="b-pager-next"]/@href'
        )
        for href in next:
            url = response.urljoin(href.extract())
            yield Request(url, callback = self.parse_story)

        loader = LEItemLoader(LEItem(), response = response)
        loader.add_xpath(
            'title', '//h1/text()'
        )
        loader.add_xpath(
            'author', '//*[@id="content"]/div[2]/span[1]/a/text()'
        )
        loader.add_value(
            'desc', ''
        )
        loader.add_xpath(
            'category', ('//*[@id="content"]/div[1]/a/text()')
        )
        loader.add_xpath(
            'body', '//*[@id="content"]/div[3]/div'
        )
        loader.add_value(
            'url', response.url
        )
        loader.add_value(
            'site', 'literotica.com'
        )
        loader.add_xpath(
            'page', '//*[@class="b-pager-active"]/text()'
        )
        yield loader.load_item()
@
% }}}

Spider for \href{https://archiveofourown.org/}{AO3}:
% ao3 spider {{{
<<ao3_spider>>=
class AOSpider(Spider):
    name = "ao"
    allowed_domains = ["archiveofourown.org"]
    start_urls = [
        "https://archiveofourown.org/media"
    ]

    def parse(self, response):
        genres = response.xpath(
            '//h3/a/@href'
        )
        for href in genres:
            url = response.urljoin(href.extract())
            yield Request(url, callback = self.parse_genre)

    def parse_genre(self, response):
        tags = response.xpath(
            '//li/ul/li/a/@href'
        )
        for href in tags:
            url = response.urljoin(href.extract())
            yield Request(url, callback = self.parse_tag)
        

    def parse_tag(self, response):
        next = response.xpath(
            '(//ol[@role="navigation"])[1]/li[last()]/a/@href'
        )
        for href in next:
            url = response.urljoin(href.extract())
            yield Request(url, callback = self.parse_tag)

        stories = response.xpath('//h4/a[1]/@href')
        for href in stories:
            extension = '?view_adult=true&style=disable'
            url = response.urljoin(href.extract()) + extension
            yield Request(url, callback = self.parse_story)

    def parse_story(self, response):
        next = response.xpath(
            'a[contains(text(), "Next Chapter â†’")]/@href'
        )
        for href in next:
            extension = '?view_adult=true&style=disable'
            url = response.urljoin(href.extract()) + extension
            yield Request(url, callback = self.parse_story)

        chapter_path   = response.xpath('//dd[@class="chapters"]/text()')
        chapters       = tuple(chapter_path.extract()[0].split('/'))
        current, total = chapters
        complete       = str(bool(current == total))

        loader = AOItemLoader(AOItem(), response = response)
        loader.add_xpath(
            'title', '//h2/text()'
        )
        loader.add_xpath(
            'author', '//a[@rel="author"]/text()'
        )
        loader.add_xpath(
            'desc', '(//*[@class="summary module"])[1]//p/text()'
        )
        loader.add_xpath(
            'body', '//*[@id="chapters"]//div/p/text()'
        )
        loader.add_value(
            'url', response.url
        )
        loader.add_value(
            'site', 'archiveofourown.org'
        )
        loader.add_xpath(
            'category', '//dd[@class="category tags"]//a/text()'
        )
        loader.add_xpath(
            'language', '//dd[@class="language"]/text()'
        )
        loader.add_xpath(
            'rating', '//dd[@class="rating tags"]//a/text()'
        )
        loader.add_xpath(
            'warnings', '//dd[@class="warning tags"]//a/text()'
        )
        loader.add_xpath(
            'fandom', '//dd[@class="fandom tags"]//a/text()'
        )
        loader.add_xpath(
            'characters', '//dd[@class="character tags"]//a/text()'
        )
        loader.add_xpath(
            'ships', '//dd[@class="relationship tags"]//a/text()'
        )
        loader.add_xpath(
            'tags', '//dd[@class="freeform tags"]//a/text()'
        )
        loader.add_xpath(
            'hits', '//dd[@class="hits"]/text()'
        )
        loader.add_xpath(
            'published', '//dd[@class="published"]/text()'
        )
        loader.add_xpath(
            'updated', '//dd[@class="status"]/text()'
        )
        loader.add_xpath(
            'words', '//dd[@class="words"]/text()'
        )
        loader.add_xpath(
            'comments', '//dd[@class="comments"]/text()'
        )
        loader.add_xpath(
            'likes', '//dd[@class="kudos"]/text()'
        )
        loader.add_xpath(
            'marks', '//dd[@class="bookmarks"]/a/text()'
        )
        loader.add_xpath(
            'hits', '//dd[@class="hits"]/text()'
        )
        loader.add_value(
            'chapter', current
        )
        loader.add_value(
            'complete', complete
        )
        yield loader.load_item()
@
% }}}

I haven't yet figured out what signal the spiders should send to avoid shutting
down the reactor (which cannot be restarted) before all three are finished (if
they are all run together).
% callback {{{
<<callback>>=
# callback fired when the spider is closed
def callback(spider, reason):
    stats = spider.crawler.stats.get_stats()  # collect/log stats?

    # stop the reactor
    reactor.stop()
@
% }}}

% settings {{{
<<settings, evaluate=False>>=
# instantiate settings and provide a custom configuration
settings = Settings()
settings.set(
    'ITEM_PIPELINES', {
        '__main__.JsonLinesExportPipeline': 100,
    }
)
settings.set(
    'USER_AGENT', 'Mozilla/5.0 (Windows NT 6.3; Win64; x64)'
)
@
% }}}

% crawl {{{
<<crawl_display, evaluate=False>>=
# instantiate a spider
ff_spider = FFSpider()
le_spider = LESpider()
ao_spider = AOSpider()

# instantiate a crawler passing in settings
# crawler = Crawler(settings)
ff_crawler = Crawler(ff_spider, settings)
le_crawler = Crawler(le_spider, settings)
ao_crawler = Crawler(ao_spider, settings)
# configure signals
ff_crawler.signals.connect(
    callback, 
    signal = signals.spider_closed
)
le_crawler.signals.connect(
    callback,
    signal = signals.spider_closed
)
ao_crawler.signals.connect(
    callback,
    signal = signals.spider_closed
)

# configure and start the crawler
# crawler.configure()
# crawler.crawl(spider)
# ff_crawler.crawl()
# le_crawler.crawl()
ao_crawler.crawl()

# start logging
# log.start()
log.configure_logging()

# start the reactor (blocks execution)
reactor.run()
@
% }}}

\section{Analysis with \href{http://www.nltk.org}{NLTK}}

% shared_objects {{{
<<shared>>=
stop = stopwords.words('english')
tokenizer = load('tokenizers/punkt/english.pickle')
@
% }}}

% real_hp {{{
<<real_hp>>=
# real

real_chapters = []
real_words    = []
dir = 'hp'

for path in glob("hp/*.txt"):
        # soup = BeautifulSoup(open(path))
        # chapter = soup.findAll(text=True)[0]
        file = open(path)
        chapter = file.read()
        chapter_tuple = (chapter, 'real')

        words = [ w.lower() for w in word_tokenize(chapter) ]

        real_chapters.append(chapter_tuple)
        real_words.extend(words)

word_total  = len(real_words)
harry_total = real_words.count('harry')

fd = FreqDist(real_words)
fd.plot(26)
@
% }}}

% real_hp_filter {{{
<<real_hp_filter>>=
# filtered_real_words = [ w.lower() for w in real_words if w.isalpha() ]
filtered_real_words = [ w for w in real_words if w not in stop ]

Rowling = filtered_real_words

fd  = FreqDist(filtered_real_words)
fd.plot(26)
@
% }}}

% ao_hp {{{
<<ao_hp>>=
file = open('ao_hp_stories.jl')

ao_chapters = []
ao_words    = []
AO3         = []
AO3_normed  = []

for line in file.readlines():
    chapter_obj = loads(line)
    if chapter_obj['language'] == 'English':
        try:
            chapter = chapter_obj['body']
            chapter_tuple = (chapter, 'fake')
            words = [ w.lower() for w in word_tokenize(chapter) ]
            ao_words.extend(words)
            ao_chapters.append(chapter_tuple)
            if len(AO3) < word_total:
                AO3.extend(words)
            if AO3_normed.count('harry') < harry_total:
                AO3_normed.extend(words)
        except:
            pass

# i = 1100
# for line in file.readlines():
#     if i > 0:
#         chapter_obj = loads(line)
#         if chapter_obj['language'] == 'English':
#             try:
#                 chapter = chapter_obj['body']
#                 chapter_tuple = (chapter, 'fake')
#                 words = word_tokenize(chapter)
#                 ao_words.extend(words)
#                 ao_chapters.append(chapter_tuple)
#                 AO3.extend(words)
#             except:
#                 pass
#     i += 1

fd = FreqDist(ao_words)
fd.plot(26)
@
% }}}

% ao_hp_filter {{{
<<ao_hp_filter>>=
# filtered_ao_words = [ w.lower() for w in ao_words if w.isalpha() ]
filtered_ao_words = [ w for w in ao_words if w not in stop ]

fd = FreqDist(filtered_ao_words)
fd.plot(26)
@
% }}}

% ff_hp {{{
<<ff_hp>>=
file = open('ff_hp_stories.jl')

ff_chapters       = []
ff_words          = []
fanfiction        = []
fanfiction_normed = []

# i = 950
for line in file.readlines():
    chapter_obj = loads(line)
    try:
        chapter = chapter_obj['body']
        chapter_tuple = (chapter, 'fake')
        words = [ w.lower() for w in word_tokenize(chapter) ]
        ff_words.extend(words)
        ff_chapters.append(chapter_tuple)
        if len(fanfiction) < word_total:
            fanfiction.extend(words)
        if fanfiction_normed.count('harry') < harry_total:
            fanfiction_normed.extend(words)
    except:
        pass

# i = 950
# for line in file.readlines():
#     if i > 0:
#         chapter_obj = loads(line)
#         try:
#             chapter = chapter_obj['body']
#             chapter_tuple = (chapter, 'fake')
#             words = word_tokenize(chapter)
#             ff_words.extend(words)
#             ff_chapters.append(chapter_tuple)
#             fanfiction.extend(words)
#         except:
#             pass
#         i -= 1

fd = FreqDist(ff_words)
fd.plot(26)
@
% }}}

% ff_hp_filter {{{
<<ff_hp_filter>>=
ff_stop = stop
for language in ('spanish', 'portuguese', 'french'):
    ff_stop.extend(
        stopwords.words(language)
    )

# filtered_ff_words = [ w.lower() for w in ff_words if w.isalpha() ]
filtered_ff_words = [ w for w in ff_words if w not in ff_stop ]

fd = FreqDist(filtered_ff_words)
fd.plot(26)
@
% }}}

% my_immortal {{{
<<my_immortal>>=
immortal_chapters = []
immortal_words    = []

for path in glob("immortal/*.txt"):
        file = open(path)
        chapter = file.read()
        chapter_tuple = (chapter, 'real')

        words = [ w.lower() for w in word_tokenize(chapter) ]

        immortal_chapters.append(chapter_tuple)
        immortal_words.extend(words)

fd = FreqDist(immortal_words)
fd.plot(26)
@
% }}}

% my_immortal_filter {{{
<<my_immortal_filter>>=
# filtered_immortal_words = [ w.lower() for w in immortal_words if w.isalpha() ]
filtered_immortal_words = [ w for w in immortal_words if w not in stop ]


fd  = FreqDist(filtered_immortal_words)
fd.plot(26)
@
% }}}

% my_immortal_extra {{{
<<my_immortal_extra>>=
i_stop = [
    'da',
    'dat',
    'u',
    'ur',
    'wif',
    'wuz',
    'chapter'
]

# filtered_immortal_words = [ w.lower() for w in immortal_words if w.isalpha() ]
filtered_immortal_words = [ w for w in immortal_words if w not in stop and w not in i_stop ]

immortal_scale = int(len(real_words) / len(immortal_words))

My_Immortal = immortal_words * immortal_scale

fd  = FreqDist(filtered_immortal_words)
fd.plot(26)
@
% }}}

% cfd {{{
<<cfd>>=
names = (
    'neville',
    'draco',
    'dumbledore',
    'fred',
    'george',
    'ginny',
    'harry',
    'hermione',
    'james',
    'lily',
    'luna',
    'lupin',
    'malfoy',
    'pansy',
    'potter',
    'remus',
    'ron',
    'severus',
    'sirius',
    'snape',
	'vamprie',
    'weasley'
)

AO3 = [ w.lower() for w in AO3 if w.isalpha() ]
AO3 = [ w for w in AO3 if w not in stop ]

fanfiction = [ w.lower() for w in fanfiction if w.isalpha() ]
fanfiction = [ w for w in fanfiction if w not in stop ]

My_Immortal = [ w.lower() for w in My_Immortal if w.isalpha() ]
My_Immortal = [ w for w in My_Immortal if w not in stop and w not in i_stop]

unified_words = (
    (source, word)
    for source in (
        'Rowling',
        'AO3',
        'fanfiction',
        'My_Immortal'
    )
    for word in eval(source)
    if word in names
)

cfd = ConditionalFreqDist(unified_words)

cfd.plot()
@

<<cfd_normed>>=
# AO3_normed = [ w.lower() for w in AO3_normed if w.isalpha() ]
# AO3_normed = [ w for w in AO3_normed if w not in stop ]

# fanfiction_normed = [ w.lower() for w in fanfiction_normed if w.isalpha() ]
# fanfiction_normed = [ w for w in fanfiction_normed if w not in stop ]

unified_words = (
    (source, word)
    for source in (
        'Rowling',
        'AO3_normed',
        'fanfiction_normed'
    )
    for word in eval(source)
	if word in names
)
cfd = ConditionalFreqDist(unified_words)
cfd.plot()
@
% }}}

\section{Training a \href{https://pypi.python.org/pypi/ChatterBot}{ChatterBot}}

% chatbot_creation {{{
<<chatbot_creation>>=
chatbot = ChatBot(
    "Mary Sue",
    io_adapter = "chatterbot.adapters.io.NoOutputAdapter"
)
# chatbot.train("chatterbot.corpus.english")
@
% }}}

% training {{{
<<training_display, evaluate=False>>=
file = open('le_products.jl')

for line in file.readlines():
    conversation = []
    story = json.loads(line)
    body = story['body']
    graphs = body.split('\n')
    for graph in graphs:
        speech = graph.split('"')[1::2]
        utterance = ' '.join(speech)
        if utterance:
            conversation.append(utterance)
    if conversation:
        chatbot.train(conversation)
@
% }}}

% conversation {{{
<<conversation, evaluate=False>>=
print(
    chatbot.get_response("Hello")
)
@
% }}}

Unfortunately, once the chatbot has been trained on a decent amount of data, it
becomes unusably slow. This is almost certainly a result of its storage method
(a flat database stored to disk) and the nature of its implementation (in
Python).

% \clearpage

% Just to test it:

% \inputminted{python}{fanfiction.py}

\end{document}
% vim: set fdm=marker fdl=0 :
